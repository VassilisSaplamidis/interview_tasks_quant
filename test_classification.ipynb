{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Test for Canditates (2024) - Classification\n",
    "\n",
    "## Objective\n",
    "\n",
    "Make a model that predicts price spikes\n",
    "\n",
    "## Task\n",
    "\n",
    "A battery operator wants to profit from the high prices in the aFRR+ energy market. The batteries however, only have limited amount of energy, and they cannot be used for more than a couple of hours before they need to be recharged.\n",
    "Luckily, the operator identified that every now and then there are \"spikes\" of the aFRR+ energy prices, that the batteries can profit from. When the aFRR+ energy price is > 350€/MWh, it's defined as a spike.\n",
    "\n",
    "These spikes are seemingly random but the operator thinks they might be correlated with the wholesale electricity prices (spot) and maybe also the solar energy production.\n",
    "\n",
    "Can you make a model that is able to relaibly predict these spikes?\n",
    "\n",
    "## Dataset\n",
    "\n",
    "A dataset with the following columns:\n",
    "- `datetime_utc_from`: Datetime in UTC (the beginning of the hour)\n",
    "- `spot_ch_eurpmwh`: The wholesale electricity price in Switzerland (€/MWh)\n",
    "- `global_radiation_J`: Average solar radiation in Switzerland (J)\n",
    "- `activation_price_pos_eurpmwh`: aFRR energy prices \n",
    "\n",
    "## Binder link\n",
    "\n",
    "https://mybinder.org/v2/gh/VassilisSaplamidis/interview_tasks_quant/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load neccesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Feature engineering\n",
    "\n",
    "### What would be good features to use for this prediction?\n",
    "- The operator thinks that the spot prices and the solar radiation play a role.\n",
    "- Can there be other time-of-day, day-of-week dependencies?\n",
    "- Can you think of other features that might correlate with the electricity consumption of any given day?\n",
    "\n",
    "You should create these features now to use them in the model afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the dataset and set the index to datetime\n",
    "\n",
    "The dataset is loaded here. Pay attention that the index is UTC time.\n",
    "We also created a second datetime column that is in the local time zone of the customers, in case you find it useful.\n",
    "\n",
    "The \"price spike\" class is also defined here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('data_raw_classification.csv', delimiter=',')\n",
    "data.set_index('datetime_utc_from', inplace=True)\n",
    "data.index = pd.to_datetime(data.index, utc=True)\n",
    "\n",
    "# add column with local time\n",
    "data['datetime_local_from'] = data.index.tz_convert('Europe/Zurich')\n",
    "\n",
    "# add column with the target \"spike\" price class\n",
    "data['pos_act_price_spike'] = data['activation_price_pos_eurpmwh'].apply(lambda x: 1 if x > 350 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create the dataset for the model\n",
    "\n",
    "1. Split the data into features (X) and target (y)<br>\n",
    "The target column should be `y = data['pos_act_price_spike']`\n",
    "2. One-hot encode categorical features (optional)<br>\n",
    "Why may this be needed?\n",
    "3. Any other preprocessing you want "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Separate features (X) and target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5', 'feature6', ...]\n",
    "target = 'pos_act_price_spike' \n",
    "X = data[features]\n",
    "y = data[target]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features\n",
    "categorical_features = ['hour', 'weekday', 'month']\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_cats = encoder.fit_transform(X[categorical_features])\n",
    "encoded_df = pd.DataFrame(encoded_cats, index=X.index, columns=encoder.get_feature_names_out(categorical_features))\n",
    "X_encoded = pd.concat([X.drop(columns=categorical_features), encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Other pre-proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Building\n",
    "\n",
    "Remember that the goal is to have a *good predictive model that is robust and can be used to predict unseen data*\n",
    "You can play around with different models, methods, objectives and parameters\n",
    "\n",
    "- What type of model did you chose? Why? \n",
    "- Does the model have any tunable parameters? How did you set their value?\n",
    "\n",
    "If you need to scale columns, the `StandardScaler` might be helpful.\n",
    "If you need to train models with different parameters/objectives etc, the `GridSearchCV` function might be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluation\n",
    "\n",
    "- Can you estimate how good your model performed? \n",
    "- Do you think it can be used to predict unseen data? Why? Why not?a\n",
    "- What improvements would you do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
